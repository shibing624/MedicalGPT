<div align="center">
  <a href="https://github.com/shibing624/MedicalGPT">
    <img src="https://github.com/shibing624/MedicalGPT/blob/main/docs/logo.png" width="120" alt="Logo">
  </a>
</div>

-----------------

# MedicalGPT: Training Medical GPT Model
[![HF Models](https://img.shields.io/badge/Hugging%20Face-shibing624-green)](https://huggingface.co/shibing624)
[![Github Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow)](https://star-history.com/#shibing624/MedicalGPT&Timeline)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)

## ğŸ“– Introduction

**MedicalGPT** training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining, 
Supervised Finetuning, Reward Modeling and Reinforcement Learning.

**MedicalGPT** è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

<img src="https://github.com/shibing624/MedicalGPT/blob/main/docs/GPT_Training.jpg" width="860" />

è®­ç»ƒé¢†åŸŸæ¨¡å‹--åŒ»ç–—æ¨¡å‹ï¼Œåˆ†å››é˜¶æ®µï¼š

- ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æ¡£æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒLLaMAæ¨¡å‹ï¼Œä»¥æ³¨å…¥é¢†åŸŸçŸ¥è¯†ï¼Œå¦‚æœ‰éœ€è¦å¯ä»¥æ‰©å……é¢†åŸŸè¯è¡¨ï¼Œæ¯”å¦‚åŒ»ç–—é¢†åŸŸè¯è¡¨
- ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾
- ç¬¬ä¸‰é˜¶æ®µï¼šRM(Reward Model)å¥–åŠ±æ¨¡å‹ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯"HHH"åŸåˆ™ï¼Œå…·ä½“æ˜¯"helpful, honest, harmless"
- ç¬¬å››é˜¶æ®µï¼šRL(Reinforcement Learning)åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬

## â–¶ï¸ Demo

- Hugging Face Demo: doing

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€æ´çš„åŸºäºgradioçš„äº¤äº’å¼webç•Œé¢ï¼Œå¯åŠ¨æœåŠ¡åï¼Œå¯é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼Œè¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹ä¼šè¿”å›ç­”æ¡ˆã€‚

1. å®‰è£…ä¾èµ–åº“ï¼š
```shell
pip install gradio
pip install mdtex2html
```

2. å¯åŠ¨æœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š
```shell
python scripts/gradio_demo.py --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir
```

å¦‚æœå·²ç»æ‰§è¡Œäº†`scripts/merge_peft_adapter.py`è„šæœ¬å°†loraæƒé‡åˆå¹¶åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œé‚£ä¹ˆæ— éœ€å†æŒ‡å®š--lora_modelï¼š
```shell
python scripts/gradio_demo.py --base_model path_to_merged_alpaca_hf_dir 
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°
- `--lora_model {lora_model}`ï¼šLoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™åªåŠ è½½--base_modelæŒ‡å®šçš„æ¨¡å‹
- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--lora_modelç›¸åŒï¼›è‹¥ä¹Ÿæœªæä¾›--lora_modelå‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ
- `--use_cpu`: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†
- `--gpus {gpu_ids}`: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2


## ğŸš€ Training Pipeline

### Stage 1: Continue Pretraining
åŸºäºllama-7bæ¨¡å‹ï¼Œä½¿ç”¨åŒ»ç–—ç™¾ç§‘ç±»æ•°æ®ç»§ç»­é¢„è®­ç»ƒï¼ŒæœŸæœ›æ³¨å…¥åŒ»ç–—çŸ¥è¯†åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¾—åˆ°llama-7b-ptæ¨¡å‹ï¼Œæ­¤æ­¥éª¤å¯é€‰

Continue pretraining of the base llama-7b model to create llama-7b-pt:

```shell
torchrun --nnodes 1 --nproc_per_node 8 scripts/run_pretraining.py \
    --model_name_or_path minlik/chinese-llama-plus-7b-merged \
    --tokenizer_name_or_path minlik/chinese-llama-plus-7b-merged \
    --dataset_name shibing624/medical \
    --dataset_config_name pretrain \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --do_train \
    --seed 42 \
    --fp16 \
    --max_train_samples 1000 \
    --max_eval_samples 10 \
    --num_train_epochs 1.0 \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_steps 50 \
    --eval_strategy steps \
    --save_steps 500 \
    --save_strategy steps \
    --save_total_limit 3 \
    --gradient_accumulation_steps 1 \
    --preprocessing_num_workers 8 \
    --block_size 1024 \
    --output_dir outputs-pt \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules q_proj,v_proj,k_proj,o_proj \
    --lora_dropout 0.05 \
    --torch_dtype float16 \
    --device_map auto \
    --gradient_checkpointing True \
    --report_to tensorboard \
    --ddp_find_unused_parameters False
```

[å‚æ•°è¯´æ˜](#å‚æ•°è¯´æ˜)

### Stage 2: Supervised FineTuning
åŸºäºllama-7b-ptæ¨¡å‹ï¼Œä½¿ç”¨åŒ»ç–—é—®ç­”ç±»æ•°æ®è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œå¾—åˆ°llama-7b-sftæ¨¡å‹

Supervised fine-tuning of the base llama-7b-pt model to create llama-7b-sft

```shell
torchrun --nnodes 1 --nproc_per_node 8 scripts/run_supervised_finetuning.py \
    --model_name_or_path <LLAMA_MODEL_PATH> \
    --tokenizer_name_or_path <LLAMA_MODEL_PATH> \
    --dataset_name shibing624/medical \
    --dataset_config_name finetune \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --do_train \
    --seed 42 \
    --fp16 \
    --max_train_samples 1000 \
    --max_eval_samples 10 \
    --num_train_epochs 5.0 \
    --learning_rate 1e-5 \
    --warmup_ratio 0.05 \
    --weight_decay 0 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_steps 50 \
    --eval_strategy steps \
    --save_steps 500 \
    --save_strategy steps \
    --save_total_limit 3 \
    --gradient_accumulation_steps 1 \
    --preprocessing_num_workers 8 \
    --max_length 512 \
    --output_dir outputs-sft \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules q_proj,v_proj,k_proj,o_proj \
    --lora_dropout 0.05 \
    --torch_dtype float16 \
    --device_map auto \
    --gradient_checkpointing True \
    --report_to tensorboard \
    --ddp_find_unused_parameters False
```

[å‚æ•°è¯´æ˜](#å‚æ•°è¯´æ˜)

### Stage 3: Reward Modeling

RM(Reward Model)ï¼šå¥–åŠ±æ¨¡å‹ï¼ŒåŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨äººç±»æ ‡æ³¨æ¥å¯¹æ¨¡å‹åš RLHF å¾®è°ƒã€‚

ç„¶è€Œï¼Œè¿™å°†éœ€è¦æˆ‘ä»¬ç»™äººç±»å‘é€ä¸€äº›æ ·æœ¬ï¼Œåœ¨æ¯è½®ä¼˜åŒ–åè®¡åˆ†ã€‚è¿™æ˜¯è´µä¸”æ…¢çš„ï¼Œå› ä¸ºæ”¶æ•›éœ€è¦çš„è®­ç»ƒæ ·æœ¬é‡å¤§ï¼Œè€Œäººç±»é˜…è¯»å’Œæ ‡æ³¨çš„é€Ÿåº¦æœ‰é™ã€‚
ä¸€ä¸ªæ¯”ç›´æ¥åé¦ˆæ›´å¥½çš„ç­–ç•¥æ˜¯ï¼Œåœ¨è¿›å…¥ RL å¾ªç¯ä¹‹å‰ç”¨äººç±»æ ‡æ³¨é›†æ¥è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹RMã€‚å¥–åŠ±æ¨¡å‹çš„ç›®çš„æ˜¯æ¨¡æ‹Ÿäººç±»å¯¹æ–‡æœ¬çš„æ‰“åˆ†ã€‚

æ„å»ºå¥–åŠ±æ¨¡å‹çš„æœ€ä½³å®è·µæ˜¯é¢„æµ‹ç»“æœçš„æ’åºï¼Œå³å¯¹æ¯ä¸ª prompt (è¾“å…¥æ–‡æœ¬) å¯¹åº”çš„ä¸¤ä¸ªç»“æœ (yk, yj)ï¼Œæ¨¡å‹é¢„æµ‹äººç±»æ ‡æ³¨çš„æ¯”åˆ†å“ªä¸ªæ›´é«˜ã€‚
RMæ¨¡å‹æ˜¯é€šè¿‡äººå·¥æ ‡æ³¨SFTæ¨¡å‹çš„æ‰“åˆ†ç»“æœæ¥è®­ç»ƒçš„ï¼Œç›®çš„æ˜¯å–ä»£äººå·¥æ‰“åˆ†ï¼Œæœ¬è´¨æ˜¯ä¸ªå›å½’æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯"HHH"åŸåˆ™ï¼Œå…·ä½“æ˜¯"helpful, honest, harmless"ã€‚


åŸºäºllama-7b-sftæ¨¡å‹ï¼Œä½¿ç”¨åŒ»ç–—é—®ç­”åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±åå¥½æ¨¡å‹ï¼Œè®­ç»ƒå¾—åˆ°llama-7b-rewardæ¨¡å‹

Reward modeling using dialog pairs from the reward dataset using the llama-7b-sft to create llama-7b-reward:

```shell
torchrun --nnodes 1 --nproc_per_node 8 scripts/run_reward_modeling.py \
    --model_name_or_path <LLAMA_SFT_MODEL> \
    --tokenizer_name_or_path <LLAMA_SFT_MODEL> \
    --dataset_name shibing624/medical \
    --dataset_config_name reward \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --do_train \
    --seed 42 \
    --fp16 \
    --max_train_samples 1000 \
    --max_eval_samples 10 \
    --num_train_epochs 5.0 \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --weight_decay 0 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_steps 50 \
    --eval_strategy steps \
    --save_steps 500 \
    --save_strategy steps \
    --save_total_limit 3 \
    --gradient_accumulation_steps 1 \
    --preprocessing_num_workers 8 \
    --max_length 512 \
    --output_dir outputs-reward \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules q_proj,v_proj,k_proj,o_proj \
    --lora_dropout 0.05 \
    --torch_dtype float16 \
    --device_map auto \
    --gradient_checkpointing True \
    --report_to tensorboard \
    --ddp_find_unused_parameters False
```
[å‚æ•°è¯´æ˜](#å‚æ•°è¯´æ˜)

### Stage 4: Reinforcement Learning

RL(Reinforcement Learning)æ¨¡å‹çš„ç›®çš„æ˜¯æœ€å¤§åŒ–å¥–åŠ±æ¨¡å‹çš„è¾“å‡ºï¼ŒåŸºäºä¸Šé¢æ­¥éª¤ï¼Œæˆ‘ä»¬æœ‰äº†å¾®è°ƒçš„è¯­è¨€æ¨¡å‹(llama-7b-sft)å’Œå¥–åŠ±æ¨¡å‹(llama-7b-reward)ï¼Œ
å¯ä»¥å¼€å§‹æ‰§è¡Œ RL å¾ªç¯äº†ã€‚

è¿™ä¸ªè¿‡ç¨‹å¤§è‡´åˆ†ä¸ºä¸‰æ­¥ï¼š

1. è¾“å…¥promptï¼Œæ¨¡å‹ç”Ÿæˆç­”å¤
2. ç”¨å¥–åŠ±æ¨¡å‹æ¥å¯¹ç­”å¤è¯„åˆ†
3. åŸºäºè¯„åˆ†ï¼Œè¿›è¡Œä¸€è½®ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ (PPO)

<img src=https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png height=400 />


åŸºäºllama-7b-rewardæ¨¡å‹ RL å¾®è°ƒè®­ç»ƒllama-7b-sftæ¨¡å‹ï¼Œå¾—åˆ°llama-7b-rlæ¨¡å‹

Reinforcement Learning fine-tuning of llama-7b-sft with the llama-7b-reward reward model to create llama-7b-rl

```shell
torchrun --nnodes 1 --nproc_per_node 8 scripts/run_rl_training.py \
    --model_name_or_path <LLAMA_SFT_MODEL> \
    --reward_model_name_or_path <LLAMA_REWARD_MODEL> \
    --tokenizer_name_or_path <LLAMA_TOKENIZER> \
    --dataset_name shibing624/medical \
    --dataset_config_name finetune \
    --validation_split_percentage 0.001 \
    --mini_batch_size 1 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --do_train \
    --seed 42 \
    --fp16 \
    --max_train_samples 1000 \
    --max_eval_samples 10 \
    --num_train_epochs 5.0 \
    --learning_rate 1.4e-5 \
    --save_steps 50 \
    --save_strategy steps \
    --gradient_accumulation_steps 1 \
    --preprocessing_num_workers 8 \
    --max_length 512 \
    --output_max_length 128 \
    --output_dir outputs-rl \
    --overwrite_output_dir \
    --logging_first_step True \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules q_proj,v_proj,k_proj,o_proj \
    --lora_dropout 0.05 \
    --torch_dtype float16 \
    --device_map auto \
    --gradient_checkpointing True \
    --report_to tensorboard \
    --early_stopping True \
    --target_kl 0.1 \
    --reward_baseline 0.0
```

### å‚æ•°è¯´æ˜

1. å¦‚æœæƒ³è¦å•å¡è®­ç»ƒï¼Œä»…éœ€å°†nproc_per_nodeè®¾ç½®ä¸º1å³å¯
2. é»˜è®¤é¢„è®­ç»ƒæ¨¡å‹æ˜¯LLaMAï¼Œå¦‚æœè®­ç»ƒå…¶ä»–GPTæ¨¡å‹ï¼Œé€‚å½“è°ƒæ•´`tokenzier_name_or_path`å’Œ`model_name_or_path`å³å¯
3. å¦‚æœè¿è¡Œç¯å¢ƒæ”¯æŒdeepspeedï¼ŒåŠ ä¸Š`--deepspeed deepspeed_config.json`
4. å¦‚æœgpuæ”¯æŒint8ï¼ŒåŠ ä¸Š`--load_in_8bit True`ä»£è¡¨é‡‡ç”¨8bité‡åŒ–è®­ç»ƒï¼Œå¯æ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨

**å…³äºLoRA Training**

é»˜è®¤ä½¿ç”¨LoRAè®­ç»ƒï¼Œæ¯ä¸ªstageçš„LoRAæ¨¡å‹æƒé‡éƒ½éœ€è¦åˆå¹¶åˆ°base modelä¸­ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆå¹¶ï¼Œä¸‹ä¸€ä¸ªstageçš„`model_name_or_path`æŒ‡å®šä¸ºåˆå¹¶åçš„æ¨¡å‹æ–‡ä»¶å¤¹ã€‚

LoRA layers were using at all stages to reduce memory requirements. 
At each stage the peft adapter layers were merged with the base model, using: 
```shell
python scripts/merge_peft_adapter.py --base_model_name_or_path X_folder --peft_model_path Y_folder --output_dir X_folder
```

- this script requires `peft>=0.3.0`
- åˆå¹¶åçš„æƒé‡ä¿å­˜åœ¨output_dirç›®å½•ä¸‹ï¼Œåç»­å¯é€šè¿‡from_pretrainedç›´æ¥åŠ è½½

**å…³äºæ¨¡å‹ç»“æœ**

è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹ä¿å­˜åœ¨output_dirç›®å½•ä¸‹ï¼Œç›®å½•ä¸‹çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š

```shell
output_dir/
|-- adapter_config.json
|-- adapter_model.bin
|-- checkpoint-24000
|   |-- adapter_config.json
|   |-- adapter_model.bin
|   |-- trainer_state.json
|   `-- training_args.bin
|-- train_results.txt
|-- eval_results.txt
|-- special_tokens_map.json
|-- tokenizer_config.json
|-- training_args.bin
â””â”€â”€ config.json
|-- logs
|   |-- 1685436851.18595
|   |   `-- events.out.tfevents.1685436851.ts-89f5028ad154472e99e7bcf2c9bf2343-launcher.82684.1
```

trainer_state.jsonè®°å½•äº†lossã€learning_rateçš„å˜åŒ–

logsç›®å½•ä¸‹çš„æ–‡ä»¶å¯ç”¨äºtensorboardå¯è§†åŒ–ï¼Œå¯åŠ¨tensorboardå‘½ä»¤å¦‚ä¸‹ï¼š
```shell
tensorboard --logdir output_dir/logs --host 0.0.0.0 --port 8008
```


**å…³äºdeepspeed**

deepspeed çš„å‚æ•°é…ç½®`deepspeed_config.json`å¯å‚è€ƒï¼š

1. https://www.deepspeed.ai/docs/config-json/
2. https://huggingface.co/docs/accelerate/usage_guides/deepspeed
3. https://github.com/huggingface/transformers/blob/main/tests/deepspeed

å¦‚æœæ˜¾å­˜å……è¶³ï¼Œå¯ä¼˜å…ˆè€ƒè™‘stage 2ï¼Œå¯¹åº”çš„é…ç½®æ–‡ä»¶æ˜¯deepspeed_config.jsonã€‚å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯é‡‡ç”¨stage 3ï¼Œè¯¥æ¨¡å¼é‡‡ç”¨æ¨¡å‹å‚æ•°å¹¶è¡Œï¼Œå¯æ˜¾è‘—å‡å°æ˜¾å­˜å ç”¨ï¼Œä½†æ˜¯è®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢å¾ˆå¤šã€‚


**å…³äºå¤šæœºå¤šå¡è®­ç»ƒ**

ä»¥ä¸¤å°æœºå™¨ä¸ºä¾‹ï¼Œæ¯å°æœºå™¨ä¸Šæœ‰8å¼ å¡

```shell
node_rank=$1
echo ${node_rank}
master_addr="10.111.112.223"

torchrun --nproc_per_node 8 --nnodes 2 --master_addr ${master_addr} --master_port 14545 --node_rank ${node_rank} srcipts/run_supervised_finetuning.py ... 
```

- node_rank ä»£è¡¨èŠ‚ç‚¹çš„rankï¼Œç¬¬ä¸€å°æœºå™¨ï¼ˆä¸»æœºå™¨ï¼‰çš„node_rankè®¾ç½®ä¸º0ï¼Œç¬¬äºŒå°æœºå™¨çš„node_rankè®¾ç½®ä¸º1
- nnodes ä»£è¡¨èŠ‚ç‚¹æœºå™¨çš„æ•°é‡
- master_addr ä»£è¡¨ä¸»æœºå™¨çš„ipåœ°å€
- master_port ä»£è¡¨ä¸ä¸»æœºå™¨é€šä¿¡çš„ç«¯å£å·


## ğŸ˜Š Inference 
è®­ç»ƒå®Œæˆåï¼Œç°åœ¨æˆ‘ä»¬åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒéªŒè¯æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ•ˆæœã€‚

```shell
python scripts/inference.py \
    --base_model path_to_llama_hf_dir \
    --lora_model path_to_lora \
    --with_prompt \
    --interactive
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
- `--lora_model {lora_model}`ï¼šLoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚å¦‚æœå·²ç»åˆå¹¶äº†LoRAæƒé‡åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™å¯ä»¥ä¸æä¾›æ­¤å‚æ•°
- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--lora_modelç›¸åŒï¼›è‹¥ä¹Ÿæœªæä¾›--lora_modelå‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ
- `--with_prompt`ï¼šæ˜¯å¦å°†è¾“å…¥ä¸promptæ¨¡ç‰ˆè¿›è¡Œåˆå¹¶ã€‚å¦‚æœåŠ è½½Alpacaæ¨¡å‹ï¼Œè¯·åŠ¡å¿…å¯ç”¨æ­¤é€‰é¡¹ï¼
- `--interactive`ï¼šä»¥äº¤äº’æ–¹å¼å¯åŠ¨ï¼Œä»¥ä¾¿è¿›è¡Œå¤šæ¬¡å•è½®é—®ç­”
- `--data_file {file_name}`ï¼šéäº¤äº’æ–¹å¼å¯åŠ¨ä¸‹ï¼ŒæŒ‰è¡Œè¯»å–file_nameä¸­çš„çš„å†…å®¹è¿›è¡Œé¢„æµ‹
- `--predictions_file {file_name}`ï¼šéäº¤äº’å¼æ–¹å¼ä¸‹ï¼Œå°†é¢„æµ‹çš„ç»“æœä»¥jsonæ ¼å¼å†™å…¥file_name
- `--use_cpu`: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†
- `--gpus {gpu_ids}`: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2




#### Inference Examples

<details>

| Human | Assitant |
| ----- | ------ |
| æ€ä¹ˆè®©è‡ªå·±ç²¾åŠ›å……æ²›ï¼Œåˆ—5ç‚¹å»ºè®® | ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥è®©ä½ ä¿æŒç²¾åŠ›å……æ²›çš„å»ºè®®ï¼š <br/>1. ç¡çœ å……è¶³ã€‚ç¡®ä¿æ¯å¤©æ™šä¸Šå¾—åˆ°å……è¶³çš„ç¡çœ æ—¶é—´ã€‚ <br/>2. å¤šè¿åŠ¨ã€‚è¿›è¡Œé€‚åº¦çš„æœ‰æ°§è¿åŠ¨ï¼Œå¦‚æ…¢è·‘ã€æ¸¸æ³³ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ æé«˜èƒ½é‡å’Œè€åŠ›ã€‚<br/>3. åƒå¥åº·é£Ÿå“ã€‚é€‰æ‹©å¯Œå«è›‹ç™½è´¨çš„é£Ÿç‰©ï¼Œå¦‚é¸¡è‚‰ã€é±¼è‚‰å’Œé¸¡è›‹ï¼Œä»¥åŠæ–°é²œçš„æ°´æœå’Œè”¬èœã€‚ <br/>4. å‡å°‘å’–å•¡å› çš„æ‘„å…¥é‡ã€‚å°½é‡é¿å…åœ¨ç™½å¤©å–å«å’–å•¡å› çš„é¥®æ–™æˆ–åƒå«æœ‰å’–å•¡å› çš„é£Ÿç‰©ã€‚ <br/>5. æ”¾æ¾èº«å¿ƒã€‚å°è¯•å†¥æƒ³ã€ç‘œä¼½æˆ–å…¶ä»–å½¢å¼çš„æ”¾æ¾æ´»åŠ¨æ¥å‡è½»å‹åŠ›å’Œç„¦è™‘æ„Ÿã€‚ |

</details>
<br/>


## ğŸ“š Dataset 

- 240ä¸‡æ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ•°æ®é›†)ï¼š[https://huggingface.co/datasets/shibing624/medical](https://huggingface.co/datasets/shibing624/medical)

## âœ… Todo

1. [ ] æ–°å¢å¤šè½®å¯¹è¯æ•°æ®å¾®è°ƒæ–¹æ³•
2. [x] add reward model finetuning
3. [x] add rl finetuning
4. [x] add medical reward dataset
5. [x] add llama in8/int4 training
6. [ ] add all training and predict demo in colab

## â˜ï¸ Contact

- Issue(å»ºè®®)
  ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat.jpeg" width="200" />

## âš ï¸ å±€é™æ€§ã€ä½¿ç”¨é™åˆ¶ä¸å…è´£å£°æ˜

åŸºäºå½“å‰æ•°æ®å’ŒåŸºç¡€æ¨¡å‹è®­ç»ƒå¾—åˆ°çš„SFTæ¨¡å‹ï¼Œåœ¨æ•ˆæœä¸Šä»å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

1. åœ¨æ¶‰åŠäº‹å®æ€§çš„æŒ‡ä»¤ä¸Šå¯èƒ½ä¼šäº§ç”Ÿè¿èƒŒäº‹å®çš„é”™è¯¯å›ç­”ã€‚

2. å¯¹äºå…·å¤‡å±å®³æ€§çš„æŒ‡ä»¤æ— æ³•å¾ˆå¥½çš„é‰´åˆ«ï¼Œç”±æ­¤ä¼šäº§ç”Ÿå±å®³æ€§è¨€è®ºã€‚

3. åœ¨ä¸€äº›æ¶‰åŠæ¨ç†ã€ä»£ç ã€å¤šè½®å¯¹è¯ç­‰åœºæ™¯ä¸‹æ¨¡å‹çš„èƒ½åŠ›ä»æœ‰å¾…æé«˜ã€‚

åŸºäºä»¥ä¸Šæ¨¡å‹å±€é™æ€§ï¼Œæˆ‘ä»¬è¦æ±‚å¼€å‘è€…ä»…å°†æˆ‘ä»¬å¼€æºçš„æ¨¡å‹æƒé‡åŠåç»­ç”¨æ­¤é¡¹ç›®ç”Ÿæˆçš„è¡ç”Ÿç‰©ç”¨äºç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºå•†ä¸šï¼Œä»¥åŠå…¶ä»–ä¼šå¯¹ç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ã€‚

æœ¬é¡¹ç›®ä»…å¯åº”ç”¨äºç ”ç©¶ç›®çš„ï¼Œé¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…å«ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚è¯¦ç»†è¯·å‚è€ƒ[å…è´£å£°æ˜](https://github.com/shibing624/MedicalGPT/blob/main/DISCLAIMER)ã€‚

é¡¹ç›®ä»£ç çš„æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œä»£ç å¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ï¼Œæ¨¡å‹æƒé‡å’Œæ•°æ®åªèƒ½ç”¨äºç ”ç©¶ç›®çš„ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ MedicalGPTçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚


## ğŸ˜‡ Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†MedicalGPTï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

```latex
@misc{MedicalGPT,
  title={MedicalGPT: Training Medical GPT Model},
  author={Ming Xu},
  year={2023},
  howpublished={\url{https://github.com/shibing624/MedicalGPT}},
}
```

## ğŸ˜ Contribute

é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

- åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
- ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

## ğŸ’• Acknowledgements 

- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

Thanks for their great work!

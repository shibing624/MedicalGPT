<div align="center">
  <a href="https://github.com/shibing624/textgen">
    <img src="https://github.com/shibing624/textgen/blob/main/docs/logo.svg" alt="Logo">
  </a>
</div>

-----------------

# TextGen: Implementation of Text Generation models
[![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
[![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)

## ğŸ“– Introduction

**TextGen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šLLaMAã€ChatGLMã€UDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚

## ğŸ˜Š Feature

- [ChatGLM](textgen/chatglm)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†ChatGLM-6Bæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¥å­çº é”™ã€å¯¹è¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [LLaMA](textgen/llama)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†LLaMAæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¯¹è¯ç”Ÿæˆä»»åŠ¡å’Œé¢†åŸŸå¾®è°ƒè®­ç»ƒ
- [BLOOM](textgen/bloom)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†BLOOMæ¨¡å‹LoRAå¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºå¯¹è¯ç”Ÿæˆä»»åŠ¡å’Œé¢†åŸŸå¾®è°ƒè®­ç»ƒ
- [UDA/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å®ç°äº†UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ã€EDAå’ŒBack Translation(å›è¯‘)ç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
- [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬

### Release Models

releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚


| Model                                                                                                     | Arch       | Introduce                                                                                                                                                                | Training                                                                                                                                     | Inference                                                                                                             | 
|:----------------------------------------------------------------------------------------------------------|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|
| [shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)             | T5         | ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹                                                                                                                                                         | [prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)                                  | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_prompt_demo.py)                       |
| [shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)                     | T5         | fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹                                                                                                                                                       | [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md) | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_couplet_demo.py)                      |
| [shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)                 | SongNet    | SongNeté¢„è®­ç»ƒæ¨¡å‹                                                                                                                                                             | -                                                                                                                                            | -                                                                                                                     |
| [shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)   | SongNet    | fine-tunedå®‹è¯åçš„æ¨¡å‹                                                                                                                                                         | [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                              | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_songci_demo.py)             |
| [shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet) | SongNet    | fine-tunedå¯¹è”åçš„æ¨¡å‹                                                                                                                                                         | [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                                 | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_couplet_demo.py)            |
| [shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)             | ChatGLM-6B | åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡                                                        | [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)                             | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)                        |
| [shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)         | ChatGLM-6B | åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡                           | [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)                       | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py) |
| [shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)           | LLaMA-13B  | åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡                            | [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)     |
| [shibing624/chinese-alpaca-plus-7b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-7b-hf)       | LLaMA-7B   | [ä¸­æ–‡LLaMA-Plus, Alpaca-Plus 7Bç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v3.0)ï¼Œåœ¨LLaMA-7Bä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨å¹¶ç»§ç»­é¢„è®­ç»ƒ120Gæ–‡æœ¬ï¼ˆé€šç”¨é¢†åŸŸï¼‰ï¼Œåœ¨4MæŒ‡ä»¤æ•°æ®é›†ä¸Šå¾®è°ƒåå¾—åˆ°çš„ä¸­æ–‡Alpaca-plusæ¨¡å‹     | [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)     |
| [shibing624/chinese-alpaca-plus-13b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf)     | LLaMA-13B  | [ä¸­æ–‡LLaMA-Plus, Alpaca-Plus 13Bç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v3.1)ï¼Œåœ¨LLaMA-13Bä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨å¹¶ç»§ç»­é¢„è®­ç»ƒ120Gæ–‡æœ¬ï¼ˆé€šç”¨é¢†åŸŸï¼‰ï¼Œåœ¨4.3MæŒ‡ä»¤æ•°æ®é›†ä¸Šå¾®è°ƒåå¾—åˆ°çš„ä¸­æ–‡Alpaca-plusæ¨¡å‹ | [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)     |

### Evaluation

| Model                                                                                                                                       | Arch       | Introduce                                                                                                                                                                                                                                                                                     | Score    |
|:--------------------------------------------------------------------------------------------------------------------------------------------|:-----------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------|
| [LLaMA-7B-Chinese-Alpaca](https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b)                                                         | LLaMA-7B   | å¤ç”¨[ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/examples/README.md)çš„è¯„ä¼°caseå’Œå¾—åˆ†                                                                                                                                                                          | 4.92     |
| [LLaMA-13B-Chinese-Alpaca](https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b)                                                       | LLaMA-13B  | å¤ç”¨[ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/examples/README.md)çš„è¯„ä¼°caseå’Œå¾—åˆ†                                                                                                                                                                          | 7.05     |
| [ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b)                                                                                       | ChatGLM-6B | åŸºäºåŸç”Ÿ`THUDM/chatglm-6b`è¯„ä¼°æµ‹è¯•é›†å¾—åˆ†                                                                                                                                                                                                                                                                 | 7.16     |
| [ChatGLM-6B-v1.1](https://huggingface.co/THUDM/chatglm-6b)                                                                                  | ChatGLM-6B | åŸºäºåŸç”Ÿ`THUDM/chatglm-6b`v1.1è‹±æ–‡ä¼˜åŒ–ç‰ˆæ¨¡å‹è¯„ä¼°æµ‹è¯•é›†å¾—åˆ†                                                                                                                                                                                                                                                      | **7.18** |
| [shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)                                           | ChatGLM-6B | åŸºäº`THUDM/chatglm-6b`åŠ è½½`shibing624/chatglm-6b-belle-zh-lora`LoRAæ¨¡å‹åè¯„ä¼°æµ‹è¯•é›†å¾—åˆ†                                                                                                                                                                                                                     | 7.03     |
| [facat/alpaca-lora-cn-13b](https://huggingface.co/facat/alpaca-lora-cn-13b)	                                                                | LLaMA-13B  | åŸºäº`decapoda-research/llama-13b-hf`åŠ è½½`facat/alpaca-lora-cn-13b`LoRAæ¨¡å‹åè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                                                               | 4.13     |  
| [Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco](https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco) | LLaMA-13B  | åŸºäº`decapoda-research/llama-13b-hf`åŠ è½½`Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco`LoRAæ¨¡å‹åè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                               | 3.98     |
| [shibing624/chinese-alpaca-plus-7b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-7b-hf)                                         | LLaMA-7B   | ä½¿ç”¨[ymcui/Chinese-LLaMA-Alpaca åˆå¹¶æ¨¡å‹æ–¹æ³•](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus)åˆå¹¶HFæƒé‡åï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ† | 6.93     |
| [shibing624/chinese-alpaca-plus-13b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf)                                       | LLaMA-13B  | ä½¿ç”¨[ymcui/Chinese-LLaMA-Alpaca åˆå¹¶æ¨¡å‹æ–¹æ³•](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus)åˆå¹¶HFæƒé‡åï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ† | 7.07     |
| [TheBloke/vicuna-13B-1.1-HF](https://huggingface.co/TheBloke/vicuna-13B-1.1-HF)                                                             | LLaMA-13B  | ä½¿ç”¨åŸç”Ÿvicuna-13B-1.1åˆå¹¶åçš„æ¨¡å‹ï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                                                                                                           | 5.13     |
| [IDEA-CCNL/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1)                                                           | LLaMA-13B  | ä½¿ç”¨å§œå­ç‰™é€šç”¨å¤§æ¨¡å‹V1ï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                                                                                                                       | 6.63     |

è¯´æ˜ï¼š
- è¯„ä¼°caseï¼Œè¯¦è§åœ¨çº¿æ–‡æ¡£ï¼šä¸­æ–‡LLM-benchmarkå¤šä»»åŠ¡è¯„ä¼°é›†(è…¾è®¯æ–‡æ¡£) https://docs.qq.com/sheet/DUUpsREtWbFBsUVJE?tab=r7io7g  æ„Ÿè°¢éŸ©ä¿Šæ˜ã€[æ¨å®¶é“­](https://github.com/yangjiam)ç­‰åŒå­¦çš„æ ‡æ³¨
- è¯„ä¼°ä»»åŠ¡ç±»å‹åŒ…æ‹¬ï¼šçŸ¥è¯†é—®ç­”ï¼Œå¼€æ”¾å¼é—®ç­”ï¼Œæ•°å€¼è®¡ç®—ï¼Œè¯—è¯ã€éŸ³ä¹ã€ä½“è‚²ï¼Œå¨±ä¹ï¼Œå†™æ–‡ç« ï¼Œæ–‡æœ¬ç¿»è¯‘ï¼Œä»£ç ç¼–ç¨‹ï¼Œä¼¦ç†ã€æ‹’ç­”ç±»ï¼Œå¤šè½®é—®ç­”ï¼ŒScore è¯„åˆ†æ˜¯å‰100æ¡ï¼ˆ10åˆ†åˆ¶ï¼‰çš„å¹³å‡åˆ†æ•°ï¼Œäººå·¥æ‰“åˆ†ï¼Œè¶Šé«˜è¶Šå¥½
- è¯„ä¼°æ•°é‡å°‘ï¼Œä»»åŠ¡ç±»å‹ä¸å¤Ÿå…¨é¢ï¼Œè¯„åˆ†ä¹‹é—´çš„å¤§å°å…³ç³»æœ‰ä¸€äº›å‚è€ƒä»·å€¼ï¼Œåˆ†æ•°çš„ç»å¯¹å€¼æ²¡å¤ªå¤§å‚è€ƒä»·å€¼
- è¯„ä¼°è„šæœ¬ï¼š[tests/test_benchmark.py](https://github.com/shibing624/textgen/blob/main/tests/test_benchmark.py) ï¼Œä½¿ç”¨fp16é¢„æµ‹ï¼Œæ— inté‡åŒ–å¤„ç†ï¼Œè¿è¡Œè„šæœ¬å¯å¤ç°è¯„ä¼°ç»“æœï¼Œä½†ç”Ÿæˆç»“æœå…·æœ‰éšæœºæ€§ï¼Œå—è§£ç è¶…å‚ã€éšæœºç§å­ç­‰å› ç´ å½±å“ã€‚è¯„æµ‹å¹¶éç»å¯¹ä¸¥è°¨ï¼Œæµ‹è¯•ç»“æœä»…ä¾›æ™¾æ™’å‚è€ƒ
- ç»“è®ºï¼šChatGLM-6Bã€LLaMA-13Bçš„ä¸­æ–‡è¡ç”Ÿæ¨¡å‹ï¼ˆåŒ…æ‹¬alpaca-plus, vicuna, ziyaï¼‰çš„è¡¨ç°å±äºç¬¬ä¸€æ¢¯é˜Ÿï¼ŒåŸç‰ˆLLaMA-7Bçš„è¡¨ç°æ•´ä½“ç¨å·®äº›
- LLaMA-13B-Chinese-Alpacaæ˜¯åœ¨åŸç‰ˆLLaMAä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œå¹¶èå…¥äº†çº¦20Gçš„é€šç”¨ä¸­æ–‡è¯­æ–™åçš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œè¡¨æ˜äº†LLaMAçš„åº•åº§ä¼˜ç§€ï¼Œå…·æœ‰å¼ºå¤§çš„è¯­è¨€è¿ç§»èƒ½åŠ›
- ChatGLMè¿™ç§åŸç”Ÿçš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹æ›´ç†è§£ä¸­æ–‡è¯­ä¹‰ï¼Œä¸”åœ¨ä¸­æ–‡çŸ¥è¯†é—®ç­”ã€å¼€æ”¾å¼é—®ç­”å¾—åˆ†é«˜
- LLaMAç³»åˆ—æ¨¡å‹æ•°å€¼è®¡ç®—ã€ä¸­è‹±ç¿»è¯‘ã€ä»£ç ç¼–ç¨‹ç±»å¾—åˆ†é«˜
- ç»è¿‡ä¸­æ–‡é¢„è®­ç»ƒå’ŒSFTå¾®è°ƒåçš„Chinese-LLaMAæ¨¡å‹åœ¨ä¸­æ–‡è¯—è¯ã€å¨±ä¹ã€ä¼¦ç†ç±»å¾—åˆ†ç›¸è¾ƒåŸç‰ˆLLaMAæœ‰æå‡

## ğŸš€ Demo

HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate

![](docs/hf.png)

run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:

```shell
python examples/gradio_demo.py
```

model trained by [examples/t5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/t5/T5_Finetune_Chinese_Couplet.ipynb)

## ğŸ’¾ Install

```shell
pip install -U textgen
```

or

install develop version:
```shell
pip install torch # conda install pytorch
git clone https://github.com/shibing624/textgen.git
cd textgen
python setup.py install
```

## â–¶ï¸ Usage

### ChatGLM-6B æ¨¡å‹

#### ä½¿ç”¨ ChatGLM-6B å¾®è°ƒåçš„æ¨¡å‹

example: [examples/chatglm/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/predict_demo.py)

```python
from textgen import ChatGlmModel

model = ChatGlmModel("chatglm", "THUDM/chatglm-6b", peft_name="shibing624/chatglm-6b-csc-zh-lora")
r = model.predict(["å¯¹ä¸‹é¢ä¸­æ–‡æ‹¼å†™çº é”™ï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚\nç­”ï¼š"])
print(r)  # ['å°‘å…ˆé˜Ÿå‘˜åº”è¯¥ä¸ºè€äººè®©åº§ã€‚\né”™è¯¯å­—ï¼šå› ï¼Œå']
```

PSï¼šç”±äºä½¿ç”¨äº†å¼€å‘ä¸­çš„peftåº“ï¼Œå¯èƒ½ç”±äºç‰ˆæœ¬æ›´æ–°ï¼Œå¯¼è‡´LoRAæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå»ºè®®ä½¿ç”¨ä¸‹é¢çš„è®­ç»ƒæ–¹æ³•ï¼Œè‡ªå·±è®­ç»ƒLoRAæ¨¡å‹ã€‚

#### è®­ç»ƒ ChatGLM-6B å¾®è°ƒæ¨¡å‹

1. æ”¯æŒè‡ªå®šä¹‰è®­ç»ƒæ•°æ®é›†å’Œè®­ç»ƒå‚æ•°ï¼Œæ•°æ®é›†æ ¼å¼å‚è€ƒ[examples/data/zh_csc_test.tsv](https://github.com/shibing624/textgen/blob/main/examples/data/zh_csc_test.tsv)æˆ–è€…[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
2. æ”¯æŒAdaLoRAã€LoRAã€P_Tuningã€Prefix_Tuningç­‰éƒ¨åˆ†å‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œä¹Ÿæ”¯æŒå…¨å‚å¾®è°ƒ
3. æ”¯æŒå¤šå¡è®­ç»ƒï¼Œæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ

example: [examples/chatglm/training_chatglm_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_demo.py)

å•å¡è®­ç»ƒï¼š
```shell
cd examples/chatglm
CUDA_VISIBLE_DEVICES=0 python training_chatglm_demo.py --do_train --do_predict --num_epochs 1 --output_dir outputs_chatglm
```

å¤šå¡è®­ç»ƒï¼š
```shell
cd examples/chatglm
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 training_chatglm_demo.py --do_train --do_predict --num_epochs 20
```


#### åŸºäºå¾®è°ƒ(LoRA)æ¨¡å‹ç»§ç»­è®­ç»ƒ
å¦‚æœéœ€è¦åŸºäºLoraæ¨¡å‹ç»§ç»­è®­ç»ƒï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬åˆå¹¶æ¨¡å‹ä¸ºæ–°çš„base modelï¼Œå†å¾®è°ƒè®­ç»ƒå³å¯ã€‚

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```shell
python -m textgen/chatglm/merge_peft_adapter.py \
    --base_model_name_or_path path_to_original_base_model_dir \
    --peft_model_path path_to_peft_model_dir \
    --output_dir path_to_output_dir 
```
å‚æ•°è¯´æ˜ï¼š
```
--base_model_name_or_pathï¼šå­˜æ”¾HFæ ¼å¼çš„åº•åº§æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
--peft_model_pathï¼šå­˜æ”¾PEFTæ ¼å¼çš„å¾®è°ƒæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
--output_dirï¼šæŒ‡å®šä¿å­˜å…¨é‡æ¨¡å‹æƒé‡çš„ç›®å½•ï¼Œé»˜è®¤ä¸º./merged
```

### LLaMA æ¨¡å‹

#### ä½¿ç”¨ LLaMA å¾®è°ƒåçš„æ¨¡å‹

example: [examples/llama/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/predict_demo.py)

<details>
<summary>show code example and result</summary>

```python
import sys

sys.path.append('../..')
from textgen import LlamaModel


def generate_prompt(instruction):
  return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:{instruction}\n\n### Response:"""


model = LlamaModel("llama", "decapoda-research/llama-7b-hf", peft_name="ziqingyang/chinese-alpaca-lora-7b")
predict_sentence = generate_prompt("é—®ï¼šç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\nç­”ï¼š")
r = model.predict([predict_sentence])
print(r)  # ['åœ°çƒæ˜¯å”¯ä¸€ä¸€é¢—æ‹¥æœ‰ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚']
```

</details>

#### è®­ç»ƒ LLaMA å¾®è°ƒæ¨¡å‹
1. æ”¯æŒè‡ªå®šä¹‰è®­ç»ƒæ•°æ®é›†å’Œè®­ç»ƒå‚æ•°ï¼Œæ•°æ®é›†æ ¼å¼å‚è€ƒ[examples/data/zh_csc_test.tsv](https://github.com/shibing624/textgen/blob/main/examples/data/zh_csc_test.tsv)æˆ–è€…[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
2. æ”¯æŒAdaLoRAã€LoRAã€P_Tuningã€Prefix_Tuningç­‰éƒ¨åˆ†å‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œä¹Ÿæ”¯æŒå…¨å‚å¾®è°ƒ
3. æ”¯æŒå¤šå¡è®­ç»ƒï¼Œæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œä½¿ç”¨æ–¹æ³•åŒä¸Šï¼ˆChatGLMå¤šå¡è®­ç»ƒï¼‰

example: [examples/llama/training_llama_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)


#### åŸºäºå¾®è°ƒ(LoRA)æ¨¡å‹ç»§ç»­è®­ç»ƒ
å¦‚æœéœ€è¦åŸºäºLoraæ¨¡å‹ç»§ç»­è®­ç»ƒï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬åˆå¹¶æ¨¡å‹ä¸ºæ–°çš„base modelï¼Œå†å¾®è°ƒè®­ç»ƒå³å¯ã€‚

å•LoRAæƒé‡åˆå¹¶ï¼ˆé€‚ç”¨äº Chinese-LLaMA, Chinese-LLaMA-Plus, Chinese-Alpacaï¼‰

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```shell
python -m textgen/llama/merge_peft_adapter.py \
    --base_model_name_or_path path_to_original_base_model_dir \
    --peft_model_path path_to_chinese_llama_or_alpaca_lora \
    --output_type [pth|huggingface]
    --output_dir path_to_output_dir 
```
å‚æ•°è¯´æ˜ï¼š
```
--base_model_name_or_pathï¼šå­˜æ”¾HFæ ¼å¼çš„åº•åº§æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
--peft_model_pathï¼šä¸­æ–‡LLaMA/Alpaca LoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HFä¸Šçš„Loraæ¨¡å‹åç§°ï¼Œå¦‚`ziqingyang/chinese-alpaca-lora-7b`ä¼šè‡ªåŠ¨ä¸‹è½½å¯¹åº”æ¨¡å‹
--output_type: æŒ‡å®šè¾“å‡ºæ ¼å¼ï¼Œå¯ä¸ºpthæˆ–huggingfaceã€‚è‹¥ä¸æŒ‡å®šï¼Œé»˜è®¤ä¸ºhuggingface
--output_dirï¼šæŒ‡å®šä¿å­˜å…¨é‡æ¨¡å‹æƒé‡çš„ç›®å½•ï¼Œé»˜è®¤ä¸º./merged
--offload_dirï¼ˆå¯é€‰ï¼‰ï¼šå¯¹äºä½å†…å­˜ç”¨æˆ·éœ€è¦æŒ‡å®šä¸€ä¸ªoffloadç¼“å­˜è·¯å¾„
```

#### è®­ç»ƒé¢†åŸŸæ¨¡å‹

| Notebook     | Description |    |
|:----------|:------------|------:|
| [training_medical_model.ipynb](https://github.com/shibing624/textgen/blob/main/examples/llama/training_medical_model.ipynb)  | è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹     |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/textgen/blob/main/examples/llama/training_medical_model.ipynb) |


### BLOOM æ¨¡å‹

#### è®­ç»ƒ BLOOM å¾®è°ƒæ¨¡å‹

example: [examples/bloom/training_bloom_demo.py](https://github.com/shibing624/textgen/blob/main/examples/bloom/training_bloom_demo.py)

### ConvSeq2Seq æ¨¡å‹

è®­ç»ƒå¹¶é¢„æµ‹ConvSeq2Seqæ¨¡å‹ï¼š

example: [examples/seq2sesq/training_convseq2seq_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_convseq2seq_model_demo.py)

<details>
<summary>show code example and result</summary>

```python
import argparse
from loguru import logger
import sys

sys.path.append('../..')
from textgen.seq2seq.conv_seq2seq_model import ConvSeq2SeqModel


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')
    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')
    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')
    parser.add_argument('--output_dir', default='./outputs/convseq2seq_zh/', type=str, help='Model output directory')
    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')
    parser.add_argument('--num_epochs', default=200, type=int, help='Number of training epochs')
    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')
    args = parser.parse_args()
    logger.info(args)

    if args.do_train:
        logger.info('Loading data...')
        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,
                                 model_dir=args.output_dir, max_length=args.max_seq_length)
        model.train_model(args.train_file)
        print(model.eval_model(args.train_file))

    if args.do_predict:
        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,
                                 model_dir=args.output_dir, max_length=args.max_seq_length)
        sentences = ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
        print("inputs:", sentences)
        print('outputs:', model.predict(sentences))


if __name__ == '__main__':
    main()
```

output:

```bash
inputs: ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„å»ºæ€ç»´çš„æœºå™¨ã€‚', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•è¿è„‘ä¸Šå·¥ä½œï¼', 'æˆ‘ä¸èƒ½é”™çƒ­æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„äººå·¥æ™ºèƒ½"200å¹´ã€‚']
```

</details>

### BART æ¨¡å‹

è®­ç»ƒå¹¶é¢„æµ‹BARTæ¨¡å‹ï¼š

example: [examples/seq2sesq/training_bartseq2seq_zh_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_bartseq2seq_zh_demo.py)

output:

```shell
inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Š', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦å—ï¼Ÿ']
```

### T5 æ¨¡å‹

example: [examples/t5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/t5/training_zh_t5_model_demo.py)

<details>
<summary>show code example and result</summary>

```python
import argparse
from loguru import logger
import pandas as pd
import sys

sys.path.append('../..')
from textgen.t5 import T5Model


def load_data(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip('\n')
            terms = line.split('\t')
            if len(terms) == 2:
                data.append(['QA', terms[0], terms[1]])
            else:
                logger.warning(f'line error: {line}')
    return data


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')
    parser.add_argument('--model_type', default='t5', type=str, help='Transformers model type')
    parser.add_argument('--model_name', default='Langboat/mengzi-t5-base', type=str, help='Transformers model or path')
    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')
    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')
    parser.add_argument('--output_dir', default='./outputs/mengzi_t5_zh/', type=str, help='Model output directory')
    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')
    parser.add_argument('--num_epochs', default=10, type=int, help='Number of training epochs')
    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')
    args = parser.parse_args()
    logger.info(args)

    if args.do_train:
        logger.info('Loading data...')
        # train_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.
        #   - `prefix`: A string indicating the task to perform. (E.g. `"question"`, `"stsb"`)
        #   - `input_text`: The input text. `prefix` is prepended to form the full input. (<prefix>: <input_text>)
        #   - `target_text`: The target sequence
        train_data = load_data(args.train_file)
        logger.debug('train_data: {}'.format(train_data[:10]))
        train_df = pd.DataFrame(train_data, columns=["prefix", "input_text", "target_text"])

        eval_data = load_data(args.train_file)[:10]
        eval_df = pd.DataFrame(eval_data, columns=["prefix", "input_text", "target_text"])

        model_args = {
            "reprocess_input_data": True,
            "overwrite_output_dir": True,
            "max_seq_length": args.max_seq_length,
            "train_batch_size": args.batch_size,
            "num_train_epochs": args.num_epochs,
            "save_eval_checkpoints": False,
            "save_model_every_epoch": False,
            "evaluate_generated_text": True,
            "evaluate_during_training": True,
            "evaluate_during_training_verbose": True,
            "use_multiprocessing": True,
            "save_best_model": True,
            "output_dir": args.output_dir,
            "use_early_stopping": True,
        }
        # model_type: t5  model_name: Langboat/mengzi-t5-base
        model = T5Model(args.model_type, args.model_name, args=model_args)

        def count_matches(labels, preds):
            logger.debug(f"labels: {labels[:10]}")
            logger.debug(f"preds: {preds[:10]}")
            match = sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])
            logger.debug(f"match: {match}")
            return match

        model.train_model(train_df, eval_data=eval_df, matches=count_matches)
        print(model.eval_model(eval_df, matches=count_matches))

    if args.do_predict:
        model = T5Model(args.model_type, args.output_dir)
        sentences = ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
        print("inputs:", sentences)
        print("outputs:", model.predict(sentences))


if __name__ == '__main__':
    main()
```

output:

```shell
inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
```

</details>

### GPT2 æ¨¡å‹

#### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ

ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚

example: [examples/gpt2/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_zh_gpt2_demo.py)

#### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ

ä½¿ç”¨ä¸­æ–‡å¯¹è”æ•°æ®é›†ï¼ˆtsvæ ¼å¼ï¼Œ`\t`é—´éš”ï¼‰ï¼Œè‡ªå®šä¹‰æ•°æ®é›†è¯»å–Datasetï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºå¯¹è”ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚

example: [examples/gpt2/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_couplet_gpt2_demo.py)

GPT2 vs T5ï¼š

1. éƒ½æ˜¯ä»Transformeræ”¹è¿›æ¥çš„ï¼ŒT5åŒæ—¶æœ‰ç¼–ç å™¨å’Œè§£ç å™¨ï¼ŒGPT2åªæœ‰è§£ç å™¨
2. T5çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯å¤„ç†ç»™å®šè¾“å…¥ï¼Œäº§å‡ºå¯¹åº”è¾“å‡ºçš„ä»»åŠ¡ï¼Œå¦‚ç¿»è¯‘ã€å¯¹è¯ã€é—®ç­”ç­‰
3. GPT2çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯è‡ªç”±åˆ›ä½œï¼Œå¦‚å†™ä¸€ç¯‡çŸ­æ–‡
4. T5çš„å¯¹è”ç”Ÿæˆæ•ˆæœå¥½äºGPT2ã€GPT2çš„è¯—è¯ç”Ÿæˆæ•ˆæœå¥½äºT5

- [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
- [å¤è¯—ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%8F%A4%E8%AF%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)

### SongNet æ¨¡å‹

æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚

example: [examples/songnet/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)

### Keyword Text Augmentation(EDA/UDA)

example: [examples/text_augmentation/text_augmentation_demo.py](examples/text_augmentation/text_augmentation_demo.py)

<details>
<summary>show code example and result</summary>

```python
import sys

sys.path.append('..')
from textgen.augment import TextAugment

if __name__ == '__main__':
    docs = ['ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹',
            'æ™šä¸Šè‚šå­å¥½éš¾å—',
            'ä½ ä¼šæ­¦åŠŸå—ï¼Œæˆ‘ä¸ä¼š',
            'ç»„è£…æ ‡é¢˜è´¨é‡å—é™äºå¹¿å‘Šä¸»è‡ªæç‰©æ–™çš„ç‰‡æ®µè´¨é‡ï¼Œä¸”è¡¨è¾¾ä¸°å¯Œåº¦æœ‰é™',
            ]
    m = TextAugment(sentence_list=docs)
    a = docs[0]
    print(a)

    b = m.augment(a, aug_ops='random-0.2')
    print('random-0.2:', b)

    b = m.augment(a, aug_ops='insert-0.2')
    print('insert-0.2:', b)

    b = m.augment(a, aug_ops='delete-0.2')
    print('delete-0.2:', b)

    b = m.augment(a, aug_ops='tfidf-0.2')
    print('tfidf-0.2:', b)

    b = m.augment(a, aug_ops='mix-0.2')
    print('mix-0.2:', b)
```

output:

```bash
ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹
random-0.2: ('ä¸»è¦é™ªé™ªæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ä¸»è¦è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå—é™äºå†…å®¹', [('ç ”ç©¶', 'é™ªé™ª', 2, 4), ('ã€', 'ä¸»è¦', 13, 15), ('ç›¸å…³', 'å—é™äº', 27, 30)])
insert-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨æœºå™¨å­¦ä¹ å­¦ä¹ ã€æ·±åº¦æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æœºå™¨', 'æœºå™¨æœºå™¨', 4, 8), ('å­¦ä¹ ', 'å­¦ä¹ å­¦ä¹ ', 8, 12), ('æ·±åº¦', 'æ·±åº¦æ·±åº¦', 13, 17)])
delete-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æ™ºèƒ½', '', 20, 20)])
tfidf-0.2: ('ä¸€æ˜¯ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½äº¤è°ˆç³»ç»Ÿå¯†åˆ‡ç›¸å…³å†…å®¹', [('ä¸»è¦', 'ä¸€æ˜¯', 0, 2), ('è§†è§‰', 'å¬è§‰', 17, 19), ('å¯¹è¯', 'äº¤è°ˆ', 22, 24), ('ç›¸å…³', 'å¯†åˆ‡ç›¸å…³', 26, 30)])
mix-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½å¯¹è¯è½¯ä»¶ç³»ç»Ÿç›¸å…³å†…å®¹', [('å­¦ä¹ ', 'å­¦', 11, 12), ('è§†è§‰', 'å¬è§‰', 16, 18), ('ç³»ç»Ÿ', 'è½¯ä»¶ç³»ç»Ÿ', 23, 27)])
```
</details>

### TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰

æ— ç›‘ç£çš„ä¸­æ–‡ç”µå•†è¯„è®ºç”Ÿæˆï¼šä»**ç”µå•†è¯„è®º**ä¸­æå–ç”¨æˆ·è¡¨è¾¾è§‚ç‚¹çš„çŸ­å¥å¹¶è¿›è¡Œç»„åˆæ¥ç”Ÿæˆä»¿çœŸè¯„è®ºã€‚

example: [examples/unsup_generation/unsup_generation_demo.py](examples/unsup_generation/unsup_generation_demo.py)

<details>
<summary>show code example and result</summary>

```python
import os
import sys

sys.path.append('..')
from textgen.unsup_generation import TglsModel, load_list

pwd_path = os.path.abspath(os.path.dirname(__file__))

samples = load_list(os.path.join(pwd_path, './data/ecommerce_comments.txt'))
docs_text = [
    ["æŒºå¥½çš„ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä¹Ÿå¾ˆå®æƒ ï¼Œä¸çŸ¥æ•ˆæœå¦‚ä½•",
     "äº§å“æ²¡å¾—è¯´ï¼Œä¹°äº†ä»¥åå°±é™ä»·ï¼Œå¿ƒæƒ…ä¸ç¾ä¸½ã€‚",
     "åˆšæ”¶åˆ°ï¼ŒåŒ…è£…å¾ˆå®Œæ•´ï¼Œä¸é”™",
     "å‘è´§é€Ÿåº¦å¾ˆå¿«ï¼Œç‰©æµä¹Ÿä¸é”™ï¼ŒåŒä¸€æ—¶é—´ä¹°çš„ä¸¤ä¸ªä¸œä¸œï¼Œä¸€ä¸ªå…ˆåˆ°ä¸€ä¸ªè¿˜åœ¨è·¯ä¸Šã€‚è¿™ä¸ªæ°´æ°´å¾ˆå–œæ¬¢ï¼Œä¸è¿‡ç›–å­çœŸçš„å¼€äº†ã€‚ç›–ä¸ç‰¢äº†ç°åœ¨ã€‚",
     "åŒ…è£…çš„å¾ˆå¥½ï¼Œæ˜¯æ­£å“",
     "è¢«ç§è‰å…°è”»ç²‰æ°´ä¸‰ç™¾å…ƒä¸€å¤§ç“¶å›¤è´§ï¼Œå¸Œæœ›æ˜¯æ­£å“å¥½ç”¨ï¼Œæ”¶åˆ°çš„æ—¶å€™ç”¨ä¿é²œè†œåŒ…è£¹å¾—ä¸¥ä¸¥å®å®ï¼Œåªæ•¢ä¹°è€ƒæ‹‰è‡ªè¥çš„æŠ¤è‚¤å“",
     ],
    ['å¾ˆæ¸©å’Œï¼Œæ¸…æ´—çš„ä¹Ÿå¾ˆå¹²å‡€ï¼Œä¸æ²¹è…»ï¼Œå¾ˆä¸é”™ï¼Œä¼šè€ƒè™‘å›è´­ï¼Œç¬¬ä¸€æ¬¡è€ƒæ‹‰ä¹°æŠ¤è‚¤å“ï¼Œæ»¡æ„',
     'è¿™æ¬¾å¸å¦†æ²¹æˆ‘ä¼šæ— é™å›è´­çš„ã€‚å³ä½¿æˆ‘æ˜¯æ²¹ç—˜çš®ï¼Œä¹Ÿä¸ä¼šé—·ç—˜ï¼ŒåŒæ—¶åœ¨è„¸éƒ¨æŒ‰æ‘©æ—¶ï¼Œè¿˜èƒ½è§£å†³ç™½å¤´çš„è„‚è‚ªç²’çš„é—®é¢˜ã€‚ç”¨æ¸…æ°´æ´—å®Œè„¸åï¼Œéå¸¸çš„æ¸…çˆ½ã€‚',
     'è‡ªä»ç”¨äº†fanclä¹‹åå°±ä¸ç”¨å…¶ä»–å¸å¦†äº†ï¼Œå¸çš„èˆ’æœåˆå¹²å‡€',
     'ä¹°è´µäº†ï¼Œå¤§æ¶¦å‘æ‰å–79ã€‚9ã€‚',
     ],
    samples
]
m = TglsModel(docs_text)
r = m.generate(samples[:500])
print('size:', len(r))
for review in r:
    print('\t' + review)
```

output:

[ç¾è¿ªæƒ å°” N.M.Fé’ˆå‰‚æ°´åº“ä¿æ¹¿é¢è†œ](https://goods.kaola.com/product/2227311.html)æœ‰å¦‚ä¸‹çš„20å¥è¯„è®ºï¼Œå…¶ä¸­æœ‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œ10å¥æ˜¯ç”Ÿæˆçš„è¯„è®ºï¼Œèƒ½çœ‹å‡ºæ¥ä¹ˆ?ğŸ˜‚

```
è¿˜ä¸é”™è¿˜ä¸é”™è¿˜ä¸é”™è¿˜ä¸é”™ã€‚
ä¸œè¥¿åˆ°äº†ï¼Œä¸çŸ¥é“å¥½ä¸å¥½ç”¨ã€‚è¯•ç”¨è¿‡åå†æ¥è¯„ä»·ã€‚åˆ°æ—¶çœ‹ç½‘è¯„éƒ½è¿˜å¯ä»¥ã€‚
å“ºä¹³æœŸå”¯ä¸€ä½¿ç”¨çš„æŠ¤è‚¤å“ï¼Œæ¯å¤©éƒ½æ˜¯ç´ é¢œï¼Œè„¸é¢å…¨é é¢è†œåŠç€ğŸ˜„è¡¥æ°´ğŸ’¦ä¸ç²˜è…»ä¸€å¦‚æ—¢å¾€çš„æ”¯æŒï¼Œå–œæ¬¢ğŸ’•
ææ´»åŠ¨æ—¶ä¹°çš„é¢è†œï¼Œä¸çŸ¥é“è¿™ä¸ªé¢è†œæ˜¯çœŸæ˜¯å‡æ•·åœ¨è„¸ä¸Šé¢è†œçº¸éƒ½æœ‰å°æ°´æ³¡é¼“èµ·æ¥ã€‚
å¾ˆä¸é”™ï¼Œéå¸¸è¡¥æ°´ï¼Œç”¨è¿‡çš„éƒ½çŸ¥é“ï¼Œæ€§ä»·æ¯”ä¹‹ç‹ï¼Œå¥½ç”¨åˆä¸è´µï¼Œæ­£å“ï¼Œç”¨ç€æ”¾å¿ƒï¼Œç‰©æµä¹Ÿå¾ˆå¿«ã€‚
é¢è†œéå¸¸å¥½ç”¨å“¦ã€‚é¢è†œè–„è–„çš„ã€‚å¥½åƒæ˜¯èš•ä¸é¢è†œå•Šã€‚ç²¾åå¾ˆå¤šå‘¢ã€‚æ•·åœ¨è„¸ä¸Šå¾ˆèˆ’æœã€‚æ„Ÿè§‰æŒºä¿æ¹¿çš„ï¼Œå‘³é“ä¹ŸæŒºå¥½é—»çš„ã€‚å°±æ˜¯é‡Œé¢åªæœ‰å•çº¯çš„é¢è†œç›´æ¥æ•·è„¸ä¸Šæœ‰ç‚¹ä¸å¥½å¼„ï¼Œå“ˆå“ˆå“ˆ
è¿˜å¯ä»¥ä¿æ¹¿æ•ˆæœä¸é”™æ°´æ¶¦æ¶¦çš„æ¯å¤©è´´ä¸€ç‰‡è„¸ä¹Ÿä¸å¹²äº†ç”¨å®Œäº†åœ¨ä¹°ç‚¹ï¼Œä¸é”™è¿˜ä¼šç»§ç»­å›è´­çš„ã€‚
å¿«é€’å¾ˆå¿«ï¼Œä¸œè¥¿å¾ˆèµï¼æƒ³è¦å¾—ç‚¹è€ƒæ‹‰è±†ä¸å®¹æ˜“ï¼Œè¿˜è¦ä¸‰åä¸ªå­—ã€‚æ—¶é—´å®è´µï¼ŒåºŸè¯ä¸è¯´ï¼ç”¨è¿‡äº†å°±çŸ¥é“äº†
æŒºå¥½ç”¨çš„ï¼Œæœ‹å‹æ¨èæ¥çš„
æŒºå¥½ç”¨çš„ï¼Œæ·¡æ·¡çš„ï¼Œè™½ç„¶ä¸æ˜¯å¾ˆæµ“ç²¾åçš„æ„Ÿè§‰ï¼Œä½†æ˜¯æ•ˆæœä¹Ÿè›®å¥½çš„ã€‚åˆ’ç®—
ä¸å¾—ä¸è¯´ç¾è¿ªæƒ å°”çš„é¢è†œæ˜¯æˆ‘ç”¨è¿‡çš„æœ€å¥½çš„é¢è†œä¹‹ä¸€ğŸ˜è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œæ²¡æƒ³åˆ°è¿™ä¹ˆä¾¿å®œçš„ä»·æ ¼ç«ŸçœŸçš„èƒ½ä¹°åˆ°çœŸå“ã€‚
ä¿æ¹¿æ•ˆæœæŒºå¥½çš„ï¼Œé¢è†œå¾ˆå¥½ç”¨ã€‚
æœŸå¾…å¥½çš„äº§å“ã€‚
ä¸€æ‰“å¼€åŒ…è£…é‡Œé¢çš„ç²¾ååˆšåˆšå¥½ï¼Œç”¨äº†è¡¥æ°´è¡¥æ°´æ•ˆæœä¸é”™ï¼Œç‰©æµéå¸¸å¿«ã€‚
çš®è‚¤å¾ˆå…‰æ»‘ğŸ˜‡æ¯”ä¸Šå»é€Ÿåº¦å¿«ä¸‰å¤©å°±åˆ°äº†ã€‚
å‰ä¸¤å¤©çš®è‚¤å¹²ç‡¥è¿ç»­æ•·äº†ä¸¤ä¸ªæ™šä¸Šæ„Ÿè§‰è¿˜ä¸é”™ğŸ˜‚è¡¥æ°´æ•ˆæœæ˜æ˜¾ï¼å¯æƒ³è€ŒçŸ¥ç²¾åæ¶²åˆå¤šå……è¶³ğŸ˜æ•·ä¸Šä»¥åå‡‰å‡‰çš„å¾ˆèˆ’æœã€‚
è¡¥æ°´æ•ˆæœä¸€èˆ¬å§ï½ä½†æ˜¯æˆ‘ç”¨çš„éŸ©å›½èƒŒå›æ¥çš„é¢è†œçº¸ä¸ç®—è–„ï¼Œå¸Œæœ›å¥½ç”¨ä¼šå›è´­çš„ï¼Œæ•·ä¸Šè„¸æ„Ÿè§‰æ¯”è¾ƒæ¸…çˆ½ï½ä»·æ ¼è¿˜ä¸ä¾¿å®œã€‚
å¸Œæœ›å¥½ç”¨ï¼Œé¢è†œç”¨è¿‡äº†å¾ˆå¥½ç”¨ï¼Œçš®è‚¤æ°´å«©å…‰æ»‘ç™½çš™ï¼Œè¡¥æ°´ä¸é”™ï¼Œä»·æ ¼ä¹Ÿåˆé€‚ã€‚
å°±æ˜¯ç²¾åæ¶²å¤ªå°‘äº†ï¼Œä¿æ¹¿æ•ˆæœä¸é”™ã€‚
é¢è†œçš„è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œä¿æ¹¿æ•ˆæœç¡®å®å¾ˆèµï¼Œè¿™ä¸ªé¢è†œç›¸å¯¹äºèƒ¶åŸè›‹ç™½å’Œç¾ç™½çš„é‚£ä¸¤æ¬¾çš„é¢è†œçº¸è¦åšä¸€äº›ï¼Œçœ‹ç€ä»·æ ¼åˆé€‚ã€‚
```

å‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œå10å¥æ˜¯ç”Ÿæˆçš„ã€‚

</details>

## ğŸ“š Dataset 

1. 50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
2. 100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
3. 5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
4. 2ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
5. 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
6. 240ä¸‡æ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®å’ŒæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†)ï¼š[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)

## âœ… Todo

1. [ ] æ–°å¢å¤šè½®å¯¹è¯æ•°æ®å¾®è°ƒæ–¹æ³•
2. [ ] add reward model finetuning
3. [ ] add rl finetuning
4. [ ] add medical reward dataset
5. [ ] add llama in4 training
6. [ ] add all training and predict demo in colab

## â˜ï¸ Contact

- Issue(å»ºè®®)
  ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="docs/wechat.jpeg" width="200" />

## ğŸ˜‡ Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

```latex
@misc{textgen,
  title={textgen: Text Generation Tool},
  author={Xu Ming},
  year={2021},
  howpublished={\url{https://github.com/shibing624/textgen}},
}
```

## ğŸ¤— License

æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚

## ğŸ˜ Contribute

é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

- åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
- ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

## ğŸ’• Acknowledgements 

- [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
- [minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn)
- [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)
- [asyml/texar](https://github.com/asyml/texar)
- [yangjianxin1/GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat)
- [williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch)
- [RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)
- [Tiiiger/bert_score](https://github.com/Tiiiger/bert_score)
- [ThilinaRajapakse/simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)
- [1YCxZ/Fake-review-generation](https://github.com/1YCxZ/Fake-review-generation)
- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)

Thanks for their great work!

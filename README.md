[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/MedicalGPT/blob/main/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/MedicalGPT/blob/main/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/MedicalGPT/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) 

<div align="center">
  <a href="https://github.com/shibing624/MedicalGPT">
    <img src="https://github.com/shibing624/MedicalGPT/blob/main/docs/logo.png" height="100" alt="Logo">
  </a>
</div>

-----------------

# MedicalGPT: Training Medical GPT Model
[![HF Models](https://img.shields.io/badge/Hugging%20Face-shibing624-green)](https://huggingface.co/shibing624)
[![Github Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow)](https://star-history.com/#shibing624/MedicalGPT&Timeline)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)

## ğŸ“– Introduction

**MedicalGPT** training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining, 
Supervised Finetuning, Reward Modeling and Reinforcement Learning.

**MedicalGPT** è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

<img src="https://github.com/shibing624/MedicalGPT/blob/main/docs/GPT_Training.jpg" width="860" />

åˆ†å››é˜¶æ®µè®­ç»ƒGPTæ¨¡å‹ï¼Œæ¥è‡ªAndrej Karpathyçš„æ¼”è®²PDF[State of GPT](https://karpathy.ai/stateofgpt.pdf)ï¼Œè§†é¢‘[Video](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)

åŸºäºæ­¤ï¼Œè®­ç»ƒé¢†åŸŸæ¨¡å‹--åŒ»ç–—æ¨¡å‹ï¼Œåˆ†å››é˜¶æ®µï¼š

- ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æ¡£æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒLLaMAæ¨¡å‹ï¼Œä»¥æ³¨å…¥é¢†åŸŸçŸ¥è¯†
- ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾
- ç¬¬ä¸‰é˜¶æ®µï¼šRM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯"HHH"åŸåˆ™ï¼Œå…·ä½“æ˜¯"helpful, honest, harmless"
- ç¬¬å››é˜¶æ®µï¼šRL(Reinforcement Learning)åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬

## â–¶ï¸ Demo

- Hugging Face Demo: doing

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€æ´çš„åŸºäºgradioçš„äº¤äº’å¼webç•Œé¢ï¼Œå¯åŠ¨æœåŠ¡åï¼Œå¯é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼Œè¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹ä¼šè¿”å›ç­”æ¡ˆã€‚

å¯åŠ¨æœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š
```shell
python scripts/gradio_demo.py --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°
- `--lora_model {lora_model}`ï¼šLoRAæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚è‹¥loraæƒé‡å·²ç»åˆå¹¶åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åˆ é™¤--lora_modelå‚æ•°
- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--lora_modelç›¸åŒï¼›è‹¥ä¹Ÿæœªæä¾›--lora_modelå‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ
- `--use_cpu`: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†
- `--gpus {gpu_ids}`: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2


## ğŸš€ Training Pipeline

Training Stage:

| Stage                           | Introduction                             | Python script                                                                                       | Shell script                                                                                 | Notebook | Colab                                                                                                                                                                                           |                     
|:--------------------------------|:-----------------------------------------|:----------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------|:---------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Stage 1: Continue Pretraining   | å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æœ¬æ•°æ®ä¸Šç»§ç»­é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥æ³¨å…¥é¢†åŸŸçŸ¥è¯†       | [scripts/pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/scripts/pretraining.py) | [scripts/run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/scripts/run_pt.sh)    | [run_pretraining.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/notebook/run_pretraining.ipynb)    | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_pretraining.ipynb)           |
| Stage 2: Supervised Fine-tuning | æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾   | [scripts/supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/scripts/supervised_finetuning.py) | [scripts/run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/scripts/run_sft.sh)  | [notebook/run_supervised_finetuning.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/notebook/run_supervised_finetuning.ipynb)     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_supervised_finetuning.ipynb) |
| Stage 3: Reward Modeling        | RM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹é½äººç±»åå¥½ï¼Œä¸»è¦æ˜¯"helpful, honest, harmless"åŸåˆ™   | [scripts/reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/scripts/reward_modeling.py) | [scripts/run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/scripts/run_rm.sh)    | [notebook/run_reward_modeling.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/notebook/run_reward_modeling.ipynb)     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_reward_modeling.ipynb)       |
| Stage 4: Reinforcement Learning | åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬   | [scripts/rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/scripts/rl_training.py) | [scripts/run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/scripts/run_rl.sh)    | [notebook/run_rl_training.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/notebook/run_rl_training.ipynb)     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_rl_training.ipynb)           |


[è®­ç»ƒå‚æ•°è¯´æ˜wiki](https://github.com/shibing624/MedicalGPT/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82%E8%AF%B4%E6%98%8E)
## ğŸ”¥ Inference 
è®­ç»ƒå®Œæˆåï¼Œç°åœ¨æˆ‘ä»¬åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒéªŒè¯æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ•ˆæœã€‚

```shell
python scripts/inference.py \
    --base_model path_to_llama_hf_dir \
    --lora_model path_to_lora \
    --with_prompt \
    --interactive
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
- `--lora_model {lora_model}`ï¼šLoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚å¦‚æœå·²ç»åˆå¹¶äº†LoRAæƒé‡åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™å¯ä»¥ä¸æä¾›æ­¤å‚æ•°
- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--lora_modelç›¸åŒï¼›è‹¥ä¹Ÿæœªæä¾›--lora_modelå‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ
- `--with_prompt`ï¼šæ˜¯å¦å°†è¾“å…¥ä¸promptæ¨¡ç‰ˆè¿›è¡Œåˆå¹¶ã€‚å¦‚æœåŠ è½½Alpacaæ¨¡å‹ï¼Œè¯·åŠ¡å¿…å¯ç”¨æ­¤é€‰é¡¹ï¼
- `--interactive`ï¼šä»¥äº¤äº’æ–¹å¼å¯åŠ¨ï¼Œä»¥ä¾¿è¿›è¡Œå¤šæ¬¡å•è½®é—®ç­”
- `--data_file {file_name}`ï¼šéäº¤äº’æ–¹å¼å¯åŠ¨ä¸‹ï¼ŒæŒ‰è¡Œè¯»å–file_nameä¸­çš„çš„å†…å®¹è¿›è¡Œé¢„æµ‹
- `--predictions_file {file_name}`ï¼šéäº¤äº’å¼æ–¹å¼ä¸‹ï¼Œå°†é¢„æµ‹çš„ç»“æœä»¥jsonæ ¼å¼å†™å…¥file_name
- `--use_cpu`: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†
- `--gpus {gpu_ids}`: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2




#### Inference Examples

<details>

| Human | Assitant |
| ----- | ------ |
| æ€ä¹ˆè®©è‡ªå·±ç²¾åŠ›å……æ²›ï¼Œåˆ—5ç‚¹å»ºè®® | ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥è®©ä½ ä¿æŒç²¾åŠ›å……æ²›çš„å»ºè®®ï¼š <br/>1. ç¡çœ å……è¶³ã€‚ç¡®ä¿æ¯å¤©æ™šä¸Šå¾—åˆ°å……è¶³çš„ç¡çœ æ—¶é—´ã€‚ <br/>2. å¤šè¿åŠ¨ã€‚è¿›è¡Œé€‚åº¦çš„æœ‰æ°§è¿åŠ¨ï¼Œå¦‚æ…¢è·‘ã€æ¸¸æ³³ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ æé«˜èƒ½é‡å’Œè€åŠ›ã€‚<br/>3. åƒå¥åº·é£Ÿå“ã€‚é€‰æ‹©å¯Œå«è›‹ç™½è´¨çš„é£Ÿç‰©ï¼Œå¦‚é¸¡è‚‰ã€é±¼è‚‰å’Œé¸¡è›‹ï¼Œä»¥åŠæ–°é²œçš„æ°´æœå’Œè”¬èœã€‚ <br/>4. å‡å°‘å’–å•¡å› çš„æ‘„å…¥é‡ã€‚å°½é‡é¿å…åœ¨ç™½å¤©å–å«å’–å•¡å› çš„é¥®æ–™æˆ–åƒå«æœ‰å’–å•¡å› çš„é£Ÿç‰©ã€‚ <br/>5. æ”¾æ¾èº«å¿ƒã€‚å°è¯•å†¥æƒ³ã€ç‘œä¼½æˆ–å…¶ä»–å½¢å¼çš„æ”¾æ¾æ´»åŠ¨æ¥å‡è½»å‹åŠ›å’Œç„¦è™‘æ„Ÿã€‚ |

</details>
<br/>


## ğŸ“š Dataset 

- 240ä¸‡æ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ•°æ®é›†)ï¼š[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)

**é™„ä¸Šä¸€äº›é€šç”¨æ•°æ®é›†å’ŒåŒ»ç–—æ•°æ®é›†çš„é“¾æ¥**

- 50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
- 100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
- 5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
- 2ä¸‡æ¡ä¸­æ–‡GPT-4æŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
- 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
- 22ä¸‡æ¡ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†(åä½—é¡¹ç›®)ï¼š[FreedomIntelligence/HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1)

## âœ… Todo

1. [ ] Added multi-round dialogue data fine-tuning method
2. [x] add reward model fine-tuning
3. [x] add rl fine-tuning
4. [x] add medical reward dataset
5. [x] add llama in8/int4 training
6. [ ] add all training and predict demo in colab

## â˜ï¸ Contact

- Issue(å»ºè®®)
  ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/MedicalGPT.svg)](https://github.com/shibing624/MedicalGPT/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="https://github.com/shibing624/MedicalGPT/blob/main/docs/wechat.jpeg" width="200" />

## âš ï¸ å±€é™æ€§ã€ä½¿ç”¨é™åˆ¶ä¸å…è´£å£°æ˜

åŸºäºå½“å‰æ•°æ®å’ŒåŸºç¡€æ¨¡å‹è®­ç»ƒå¾—åˆ°çš„SFTæ¨¡å‹ï¼Œåœ¨æ•ˆæœä¸Šä»å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

1. åœ¨æ¶‰åŠäº‹å®æ€§çš„æŒ‡ä»¤ä¸Šå¯èƒ½ä¼šäº§ç”Ÿè¿èƒŒäº‹å®çš„é”™è¯¯å›ç­”ã€‚

2. å¯¹äºå…·å¤‡å±å®³æ€§çš„æŒ‡ä»¤æ— æ³•å¾ˆå¥½çš„é‰´åˆ«ï¼Œç”±æ­¤ä¼šäº§ç”Ÿå±å®³æ€§è¨€è®ºã€‚

3. åœ¨ä¸€äº›æ¶‰åŠæ¨ç†ã€ä»£ç ã€å¤šè½®å¯¹è¯ç­‰åœºæ™¯ä¸‹æ¨¡å‹çš„èƒ½åŠ›ä»æœ‰å¾…æé«˜ã€‚

åŸºäºä»¥ä¸Šæ¨¡å‹å±€é™æ€§ï¼Œæˆ‘ä»¬è¦æ±‚å¼€å‘è€…ä»…å°†æˆ‘ä»¬å¼€æºçš„æ¨¡å‹æƒé‡åŠåç»­ç”¨æ­¤é¡¹ç›®ç”Ÿæˆçš„è¡ç”Ÿç‰©ç”¨äºç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºå•†ä¸šï¼Œä»¥åŠå…¶ä»–ä¼šå¯¹ç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ã€‚

æœ¬é¡¹ç›®ä»…å¯åº”ç”¨äºç ”ç©¶ç›®çš„ï¼Œé¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…å«ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚è¯¦ç»†è¯·å‚è€ƒ[å…è´£å£°æ˜](https://github.com/shibing624/MedicalGPT/blob/main/DISCLAIMER)ã€‚

é¡¹ç›®ä»£ç çš„æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œä»£ç å¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ï¼Œæ¨¡å‹æƒé‡å’Œæ•°æ®åªèƒ½ç”¨äºç ”ç©¶ç›®çš„ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ MedicalGPTçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚


## ğŸ˜‡ Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†MedicalGPTï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

```latex
@misc{MedicalGPT,
  title={MedicalGPT: Training Medical GPT Model},
  author={Ming Xu},
  year={2023},
  howpublished={\url{https://github.com/shibing624/MedicalGPT}},
}
```

## ğŸ˜ Contribute

é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

- åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
- ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

## ğŸ’• Acknowledgements 

- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

Thanks for their great work!

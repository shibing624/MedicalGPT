## è®­ç»ƒè„šæœ¬


- ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒ `run_pt.sh`
- ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒ `run_sft.sh`
- ç¬¬ä¸‰é˜¶æ®µ
  - RLHF(Reinforcement Learning from Human Feedback)åˆ†ä¸ºä¸¤æ­¥ï¼š
    - RM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ `run_rm.sh`
    - RL(Reinforcement Learning)åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  `run_ppo.sh`
  - DPO(Direct Preference Optimization)ç›´æ¥åå¥½ä¼˜åŒ– `run_dpo.sh`


## è®­ç»ƒå‚æ•°è¯´æ˜

1. å¦‚æœæƒ³è¦å•å¡è®­ç»ƒï¼Œä»…éœ€å°†nproc_per_nodeè®¾ç½®ä¸º1å³å¯ï¼Œæˆ–è€…å»æ‰torchrunå‘½ä»¤ï¼Œç›´æ¥è¿è¡Œpythonè„šæœ¬ï¼Œå¦‚`python supervised_finetuning.py`
2. æŒ‡å®šè®­ç»ƒçš„baseæ¨¡å‹ï¼ˆé»˜è®¤llamaï¼‰ï¼Œè®­ç»ƒä»£ç ä¹Ÿå…¼å®¹ChatGLM/BLOOM/BaiChuanç­‰GPTæ¨¡å‹ï¼Œä»¥baichuanæ¨¡å‹ä¸ºä¾‹ï¼Œè°ƒæ•´`--model_name_or_path baichuan-inc/Baichuan-13B-Chat`ï¼Œç‰¹åˆ«çš„ï¼Œå¦‚æœæœªè®­ç»ƒåªæ¨ç†ï¼Œbase modelæ˜¯ç±»ä¼¼`baichuan-inc/Baichuan-13B-Chat`å·²ç»å¯¹é½çš„æ¨¡å‹ï¼Œåˆ™éœ€è¦æŒ‡å®š`--template_name baichuan`ï¼›å¦‚æœåœ¨base modelåŸºç¡€ä¸Šè®­ç»ƒï¼Œé»˜è®¤é‡‡ç”¨`vicuna`æ¨¡æ¿ï¼Œåç»­ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¨ç†æ—¶ï¼Œä¹ŸæŒ‡å®šç›¸åŒçš„`--template_name vicuna`å³å¯
3. æŒ‡å®šè®­ç»ƒé›†ï¼Œ`--train_file_dir`æŒ‡å®šè®­ç»ƒæ•°æ®ç›®å½•ï¼Œ`--validation_file_dir`æŒ‡å®šéªŒè¯æ•°æ®ç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨`--dataset_name`æŒ‡å®šçš„HF datasetsæ•°æ®é›†ï¼Œè®­ç»ƒé›†å­—æ®µæ ¼å¼è§[æ•°æ®é›†æ ¼å¼](https://github.com/shibing624/MedicalGPT/wiki/%E6%95%B0%E6%8D%AE%E9%9B%86)ï¼Œå»ºè®®é¢†åŸŸè®­ç»ƒé›†ä¸­åŠ å…¥ä¸€äº›é€šç”¨å¯¹è¯æ•°æ®ï¼Œæ•°æ®é›†é“¾æ¥è§[ğŸ“š Dataset](https://github.com/shibing624/MedicalGPT#-dataset)ï¼Œå½“å‰é»˜è®¤å¤šè½®å¯¹è¯æ ¼å¼ï¼Œå…¼å®¹å•è½®å¯¹è¯ï¼Œå¾®è°ƒè®­ç»ƒé›†å¦‚æœæ˜¯alpacaæ ¼å¼ï¼Œå¯ä»¥ç”¨[convert_dataset.py](https://github.com/shibing624/MedicalGPT/blob/main/convert_dataset.py)è½¬ä¸ºshareGPTæ ¼å¼ï¼Œå³å¯ä¼ å…¥è®­ç»ƒ
4. å¦‚æœè¿è¡Œç¯å¢ƒæ”¯æŒdeepspeedï¼ŒåŠ ä¸Š`--deepspeed zero2.json`å‚æ•°å¯åŠ¨zero2æ¨¡å¼ï¼›æ˜¾å­˜ä¸è¶³ï¼ŒåŠ ä¸Š`--deepspeed zero3.json --fp16`å‚æ•°å¯åŠ¨zero3æ··åˆç²¾åº¦æ¨¡å¼
5. å¦‚æœgpuæ”¯æŒint8/int4é‡åŒ–ï¼ŒåŠ ä¸Š`--load_in_4bit True`ä»£è¡¨é‡‡ç”¨4bité‡åŒ–è®­ç»ƒï¼Œæˆ–è€…`--load_in_8bit True`ä»£è¡¨é‡‡ç”¨8bité‡åŒ–è®­ç»ƒï¼Œå‡å¯æ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨
6. è®­ç»ƒé›†æ¡æ•°æ§åˆ¶ï¼Œ`--max_train_samples`å’Œ`--max_eval_samples`æŒ‡å®šè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°ï¼Œç”¨äºå¿«é€ŸéªŒè¯ä»£ç æ˜¯å¦å¯ç”¨ï¼Œè®­ç»ƒæ—¶å»ºè®®è®¾ç½®ä¸º`--max_train_samples -1`è¡¨ç¤ºç”¨å…¨éƒ¨è®­ç»ƒé›†ï¼Œ`--max_eval_samples 50`è¡¨ç¤ºç”¨50æ¡éªŒè¯æ•°æ®
7. è®­ç»ƒæ–¹å¼ï¼ŒæŒ‡å®š`--use_peft False`ä¸ºå…¨å‚è®­ç»ƒï¼ˆè¦ç§»é™¤`--fp16`ï¼‰ï¼Œ`--use_peft True`æ˜¯LoRAè®­ç»ƒï¼›æ³¨æ„ï¼šå…¨å‚è®­ç»ƒLLaMA-7Bæ¨¡å‹éœ€è¦120GBæ˜¾å­˜ï¼ŒLoRAè®­ç»ƒéœ€è¦13GBæ˜¾å­˜
8. æ”¯æŒæ¢å¤è®­ç»ƒï¼ŒLoRAè®­ç»ƒæ—¶æŒ‡å®š`--peft_path`ä¸ºæ—§çš„adapter_model.binæ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ï¼›å…¨å‚è®­ç»ƒæ—¶æŒ‡å®š`--resume_from_checkpoint`ä¸ºæ—§æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
9. PTå’ŒSFTæ”¯æŒqloraè®­ç»ƒï¼Œå¦‚æœä½¿ç”¨çš„æ˜¯ RTX4090ã€A100 æˆ– H100 GPUï¼Œæ”¯æŒnf4ï¼Œä½¿ç”¨`--qlora True --load_in_4bit True`å‚æ•°å¯ç”¨qloraè®­ç»ƒï¼Œå¼€å¯qloraè®­ç»ƒï¼Œä¼šå‡å°‘æ˜¾å­˜å ç”¨ï¼Œè®­ç»ƒåŠ é€Ÿï¼ŒåŒæ—¶å»ºè®®è®¾ç½®`--torch_dtype bfloat16 --optim paged_adamw_32bit`ä¿è¯è®­ç»ƒç²¾åº¦
10. æ‰©è¯è¡¨åçš„å¢é‡é¢„è®­ç»ƒï¼ŒPTé˜¶æ®µåŠ ä¸Š`--modules_to_save embed_tokens,lm_head`å‚æ•°ï¼Œåç»­SFTç­‰é˜¶æ®µä¸ç”¨åŠ 
11. æ–°å¢äº†RoPEæ’å€¼æ¥æ‰©å±•GPTæ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé€šè¿‡[ä½ç½®æ’å€¼æ–¹æ³•](https://arxiv.org/abs/2306.15595)ï¼Œåœ¨å¢é‡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹è·å¾—é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œä½¿ç”¨ `--rope_scaling linear` å‚æ•°è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨`--rope_scaling dynamic` å‚æ•°é¢„æµ‹æ¨¡å‹
12. é’ˆå¯¹LLaMAæ¨¡å‹æ”¯æŒäº†[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ RTX3090ã€RTX4090ã€A100 æˆ– H100 GPUï¼ŒSFTä¸­è¯·ä½¿ç”¨ `--flash_attn` å‚æ•°ä»¥å¯ç”¨ FlashAttention-2
13. æ–°å¢äº†[LongLoRA](https://github.com/dvlab-research/LongLoRA) æå‡ºçš„ **$S^2$-Attn**ï¼Œä½¿æ¨¡å‹è·å¾—é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼ŒSFTä¸­ä½¿ç”¨ `--shift_attn` å‚æ•°ä»¥å¯ç”¨è¯¥åŠŸèƒ½
14. æ”¯æŒäº†[NEFTune](https://github.com/neelsjain/NEFTune)ç»™embeddingåŠ å™ªSFTè®­ç»ƒæ–¹æ³•ï¼Œ[NEFTune paper](https://arxiv.org/abs/2310.05914), SFTä¸­ä½¿ç”¨ `--neft_alpha` å‚æ•°å¯ç”¨ NEFTuneï¼Œä¾‹å¦‚ `--neft_alpha 5`
15. æ”¯æŒå¾®è°ƒMixtralæ··åˆä¸“å®¶MoEæ¨¡å‹ **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**ï¼ŒSFTä¸­å¦‚æœç”¨loraå¾®è°ƒæ¨¡å‹ï¼Œå¯ä»¥å¼€å¯4bité‡åŒ–å’ŒQLoRA`--load_in_4bit True --qlora True`ä»¥èŠ‚çœæ˜¾å­˜ï¼Œå»ºè®®è®¾ç½®`--target_modules q_proj,k_proj,v_proj,o_proj`ï¼Œè¿™æ ·å¯ä»¥é¿å…å¯¹MoEä¸“å®¶ç½‘ç»œçš„MLPå±‚é‡åŒ–ï¼Œå› ä¸ºå®ƒä»¬å¾ˆç¨€ç–ä¸”é‡åŒ–åä¼šå¯¼è‡´æ€§èƒ½æ•ˆæœä¸‹é™ã€‚


**å…³äºLoRA Training**

é»˜è®¤ä½¿ç”¨LoRAè®­ç»ƒï¼Œæ¯ä¸ªstageçš„LoRAæ¨¡å‹æƒé‡éƒ½éœ€è¦åˆå¹¶åˆ°base modelä¸­ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆå¹¶ï¼Œä¸‹ä¸€ä¸ªstageçš„`model_name_or_path`æŒ‡å®šä¸ºåˆå¹¶åçš„æ¨¡å‹æ–‡ä»¶å¤¹ã€‚

LoRA layers were using at all stages to reduce memory requirements.
At each stage the peft adapter layers were merged with the base model, using:
```shell
python merge_peft_adapter.py \
  --base_model base_model_dir \
  --tokenizer_path base_model_dir \
  --lora_model lora_model_dir \
  --output_dir outputs-merged
```

- this script requires `peft>=0.4.0`
- åˆå¹¶åçš„æƒé‡ä¿å­˜åœ¨output_dirç›®å½•ä¸‹ï¼Œåç»­å¯é€šè¿‡from_pretrainedç›´æ¥åŠ è½½

**å…³äºæ¨¡å‹ç»“æœ**

è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹ä¿å­˜åœ¨output_dirç›®å½•ä¸‹ï¼Œç›®å½•ä¸‹çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š

```shell
output_dir/
|-- adapter_config.json
|-- adapter_model.bin
|-- checkpoint-24000
|   |-- adapter_config.json
|   |-- adapter_model.bin
|   |-- trainer_state.json
|   `-- training_args.bin
|-- train_results.txt
|-- eval_results.txt
|-- special_tokens_map.json
|-- tokenizer_config.json
|-- training_args.bin
|-- logs
|   |-- 1685436851.18595
|   |   `-- events.out.tfevents.1685436851.ts-89f5028ad154472e99e7bcf2c9bf2343-launcher.82684.1
â””â”€â”€ config.json

```

- `trainer_state.json`è®°å½•äº†lossã€learning_rateçš„å˜åŒ–
- logsç›®å½•ä¸‹çš„æ–‡ä»¶å¯ç”¨äºtensorboardå¯è§†åŒ–ï¼Œå¯åŠ¨tensorboardå‘½ä»¤å¦‚ä¸‹ï¼š
```shell
tensorboard --logdir output_dir/logs --host 0.0.0.0 --port 8008
```


**å…³äºdeepspeed**

deepspeed çš„å‚æ•°é…ç½®`deepspeed_config.json`å¯å‚è€ƒï¼š

1. https://www.deepspeed.ai/docs/config-json/
2. https://huggingface.co/docs/accelerate/usage_guides/deepspeed
3. https://github.com/huggingface/transformers/blob/main/tests/deepspeed

å¦‚æœæ˜¾å­˜å……è¶³ï¼Œå¯ä¼˜å…ˆè€ƒè™‘stage 2ï¼Œå¯¹åº”çš„é…ç½®æ–‡ä»¶æ˜¯`deepspeed_zero_stage2_config.json`ã€‚å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯é‡‡ç”¨stage 3ï¼Œå¯¹åº”çš„é…ç½®æ–‡ä»¶æ˜¯`deepspeed_zero_stage3_config.json`ï¼Œè¯¥æ¨¡å¼é‡‡ç”¨æ¨¡å‹å‚æ•°å¹¶è¡Œï¼Œå¯æ˜¾è‘—å‡å°æ˜¾å­˜å ç”¨ï¼Œä½†æ˜¯è®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢å¾ˆå¤šã€‚


**å…³äºå¤šæœºå¤šå¡è®­ç»ƒ**

ä»¥ä¸¤å°æœºå™¨ä¸ºä¾‹ï¼Œæ¯å°æœºå™¨ä¸Šæœ‰8å¼ å¡

```shell
node_rank=$1
echo ${node_rank}
master_addr="10.111.112.223"

torchrun --nproc_per_node 8 --nnodes 2 --master_addr ${master_addr} --master_port 14545 --node_rank ${node_rank} run_supervised_finetuning.py ...
```


- node_rank ä»£è¡¨èŠ‚ç‚¹çš„rankï¼Œç¬¬ä¸€å°æœºå™¨ï¼ˆä¸»æœºå™¨ï¼‰çš„node_rankè®¾ç½®ä¸º0ï¼Œç¬¬äºŒå°æœºå™¨çš„node_rankè®¾ç½®ä¸º1
- nnodes ä»£è¡¨èŠ‚ç‚¹æœºå™¨çš„æ•°é‡
- master_addr ä»£è¡¨ä¸»æœºå™¨çš„ipåœ°å€
- master_port ä»£è¡¨ä¸ä¸»æœºå™¨é€šä¿¡çš„ç«¯å£å·

ä»¥ä¸Šå‘½ä»¤åœ¨ä¸¤å°æœºå™¨å„æ‰§è¡Œä¸€æ¬¡ï¼Œä¸¤å°æœºå™¨çš„node_rankè®¾ç½®ä¸åŒã€‚
